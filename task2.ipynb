{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter absolute directory path to data files  => C:\\ Users\\....... \\ sample data\\ Z\n",
      "C:\\Users\\abhee\\Desktop\\asu\\cse 515(mwdb)\\project\\phase1\\sample_data\\Z\n",
      "task2 completed\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "\n",
    "print('Enter absolute directory path to data files  => C:\\ Users\\....... \\ sample data\\ Z')\n",
    "directory = input()\n",
    "# directory =  \"C:\\\\Users\\\\abhee\\\\Desktop\\\\asu\\\\cse 515(mwdb)\\\\project\\\\phase1\\\\sample_data\\\\Z\"\n",
    "\n",
    "# change the current directory to data files directory\n",
    "os.chdir(directory)\n",
    "all_files = glob.glob('*.csv')\n",
    "total_files = len(all_files)\n",
    "# total_files = 60\n",
    "total_sensors = 20\n",
    "vector_file = open(\"vectors.txt\", \"w\")\n",
    "\n",
    "file_string = ''\n",
    "# read all files\n",
    "for filename in range(1, total_files+1):\n",
    "    word_file = open(\"{0}.wrd\".format(filename), \"r\")\n",
    "    file_string += word_file.read()\n",
    "    if filename < total_files:\n",
    "        file_string += '\\n'\n",
    "    word_file.close()\n",
    "\n",
    "data_list = file_string.split('\\n')\n",
    "\n",
    "# get all words from all files and store them in a set -- set is used to remove duplicates --\n",
    "feature_set = set()\n",
    "\n",
    "# total count of words in each file\n",
    "word_count = [0]*(total_files+1)\n",
    "\n",
    "for word in data_list:\n",
    "    word_list = word.split(',')\n",
    "    curr_file = int(word_list[0])\n",
    "    feature = word_list[3]\n",
    "    word_count[curr_file] += 1\n",
    "    feature_set.add(feature)\n",
    "\n",
    "# convert the set to list to maintain the order in which features occur\n",
    "feature_list1 = list(feature_set)\n",
    "\n",
    "# tf_list stores the tf values for all files in the directory\n",
    "tf_list = [[0]*len(feature_list1) for _ in range(total_files+1)]\n",
    "\n",
    "# number of times a given feature is present in a file\n",
    "for word in data_list:\n",
    "    word_list = word.split(',')\n",
    "    feature = word_list[3]\n",
    "    curr_file = int(word_list[0])\n",
    "    index = feature_list1.index(feature)\n",
    "    tf_list[curr_file][index] += 1\n",
    "\n",
    "# get tf values = n/K\n",
    "for i in range(1, len(tf_list)):\n",
    "    for j in range(len(tf_list[0])):\n",
    "        tf_list[i][j] = tf_list[i][j]/word_count[i]\n",
    "    \n",
    "# write tf to a file\n",
    "tf_str = ''\n",
    "for i in range(1, len(tf_list)):\n",
    "    tf_str += 'TF,' + str(i) + ',' \n",
    "    for j in range(len(tf_list[0])):\n",
    "        tf_str += str(tf_list[i][j])\n",
    "        if j < len(tf_list[0])-1:\n",
    "            tf_str += ','\n",
    "    tf_str += '\\n'\n",
    "vector_file.write(tf_str)\n",
    "\n",
    "# compute idf -- each feature in the database will have an idf value -- \n",
    "idf = [0]*len(feature_list1)\n",
    "prev_file = 0\n",
    "feature_set = set()\n",
    "\n",
    "# increment the idf count for each unique feature in a file\n",
    "for word in data_list:\n",
    "    word_list = word.split(',')\n",
    "    feature = word_list[3]\n",
    "    curr_file = int(word_list[0])\n",
    "    if curr_file != prev_file:\n",
    "        feature_set = set()\n",
    "        prev_file = curr_file \n",
    "    if feature not in feature_set:\n",
    "        feature_set.add(feature)\n",
    "        index = feature_list1.index(feature)\n",
    "        idf[index] += 1\n",
    "\n",
    "# idf = log(N/m)\n",
    "for i in range(len(idf)):\n",
    "    idf[i] = math.log10(total_files/idf[i])\n",
    "\n",
    "# tf_idf = tf*idf\n",
    "tfidf = [[0]*len(feature_list1) for _ in range(total_files+1)]\n",
    "\n",
    "for i in range(1, len(tfidf)):\n",
    "    for j in range(len(tfidf[0])):\n",
    "         tfidf[i][j] = tf_list[i][j]*idf[j]\n",
    "\n",
    "# write tf_idf to a file\n",
    "tfidf_str = ''\n",
    "for i in range(1, len(tfidf)):\n",
    "    tfidf_str += 'TFIDF,' + str(i) + ',' \n",
    "    for j in range(len(tfidf[0])):\n",
    "        tfidf_str += str(tfidf[i][j])\n",
    "        if j < len(tfidf[0])-1:\n",
    "            tfidf_str += ','\n",
    "    tfidf_str += '\\n'\n",
    "vector_file.write(tfidf_str)\n",
    "\n",
    "# compute tfidf2 -- tfidf2 represents discriminatory power of features among sensors --\n",
    "prev_sensor = 0\n",
    "idf2 = [0]*len(feature_list1)\n",
    "feature_set = set()\n",
    "\n",
    "# increment the idf count for each unique feature in a file\n",
    "for word in data_list:\n",
    "    word_list = word.split(',')\n",
    "    feature = word_list[3]\n",
    "    curr_sensor = int(word_list[1])\n",
    "    if curr_sensor != prev_sensor:\n",
    "        feature_set = set()\n",
    "        prev_sensor = curr_sensor \n",
    "    if feature not in feature_set:\n",
    "        feature_set.add(feature)\n",
    "        index = feature_list1.index(feature)\n",
    "        idf2[index] += 1\n",
    "\n",
    "# idf2 = log(N/m)\n",
    "N = total_files*total_sensors\n",
    "for i in range(len(idf2)):\n",
    "    idf2[i] = math.log10(N/idf2[i])\n",
    "\n",
    "# tf_idf = tf*idf2\n",
    "tfidf2 = [[0]*len(feature_list1) for _ in range(total_files+1)]\n",
    "\n",
    "for i in range(1, len(tfidf2)):\n",
    "    for j in range(len(tfidf2[0])):\n",
    "         tfidf2[i][j] = tf_list[i][j]*idf2[j]\n",
    "\n",
    "# write tf_idf2 to a file\n",
    "tfidf_str2 = ''\n",
    "for i in range(1, len(tfidf2)):\n",
    "    tfidf_str2 += 'TFIDF2,' + str(i) + ',' \n",
    "    for j in range(len(tfidf2[0])):\n",
    "        tfidf_str2 += str(tfidf2[i][j])\n",
    "        if j < len(tfidf2[0])-1:\n",
    "            tfidf_str2 += ','\n",
    "    tfidf_str2 += '\\n'\n",
    "vector_file.write(tfidf_str2)\n",
    "\n",
    "# store feature list1 in vector.txt\n",
    "featurelist1_s = 'FeatureList1,0,'\n",
    "for i in range(len(feature_list1)):\n",
    "    featurelist1_s += str(feature_list1[i]) \n",
    "    if i < len(feature_list1)-1:\n",
    "        featurelist1_s += ','\n",
    "vector_file.write(featurelist1_s)\n",
    "\n",
    "print('task2 completed')\n",
    "\n",
    "vector_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
